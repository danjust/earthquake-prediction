{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into 150,000 point segments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_segments = int(len(data['acoustic_data'])/150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segment = {}\n",
    "\n",
    "ind_save = 0\n",
    "for i in range(num_segments):\n",
    "    if data['time_to_failure'].values[i*150000] > data['time_to_failure'].values[(i+1)*150000-1]:\n",
    "        segment['data'] = data['acoustic_data'].values[i*150000:(i+1)*150000]\n",
    "        segment['time_to_failure'] = data['time_to_failure'].values[(i+1)*150000]\n",
    "        with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'%ind_save, 'wb') as f:\n",
    "            pickle.dump(segment,f)\n",
    "        ind_save = ind_save+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_segments_use = ind_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spectrograms for segments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = 4000000\n",
    "sampling_period = 1 / fs\n",
    "\n",
    "spectrum = {}\n",
    "\n",
    "for i in range(num_segments_use):\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'%i, 'rb') as f:\n",
    "        segment = pickle.load(f)\n",
    "    _,_, s = signal.spectrogram(segment['data'], fs, window=('tukey', .25))\n",
    "    spectrum['data'] = s[:33,:]\n",
    "    spectrum['time_to_failure'] = segment['time_to_failure']\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/spectrograms/spectrum_%04d.pkl'%i, 'wb') as f:\n",
    "        pickle.dump(spectrum,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features for segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureselection = ['mean', 'min', '10th percentile', 'median', '90th percentile', 'max', \n",
    "                    'stdev', 'skewness', 'kurtosis',\n",
    "                    'mean spectral power 1500-2999', 'max spectral power 1500-2999',\n",
    "                    'mean spectral power 3000-4499', 'max spectral power 3000-4499',\n",
    "                    'mean spectral power 4500-5999', 'max spectral power 4500-5999',\n",
    "                    'mean spectral power 6000-7499', 'max spectral power 6000-7499',\n",
    "                    'mean spectral power 7500-8999', 'max spectral power 7500-8999',\n",
    "                    'mean spectral power 9000-10499', 'max spectral power 9000-10499',\n",
    "                    'mean spectral power 10500-11999', 'max spectral power 10500-11999',\n",
    "                    'mean spectral power 12000-13499', 'max spectral power 12000-13499',\n",
    "                    'mean spectral power 13500-15000', 'max spectral power 13500-15000',\n",
    "                    'location of 1st of 3 highest peaks', 'amplitude of 1st of 3 highest peaks',\n",
    "                    'location of 2nd of 3 highest peaks', 'amplitude of 2nd of 3 highest peaks',\n",
    "                    'location of 3rd of 3 highest peaks', 'amplitude of 3rd of 3 highest peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "featuremat = np.zeros([num_segments_use,33])\n",
    "time_to_failure_vec = np.zeros(num_segments_use)\n",
    "\n",
    "for segment_index in range(num_segments_use):\n",
    "    featurevec = []\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'\n",
    "              %segment_index, 'rb') as f:\n",
    "        segment = pickle.load(f)\n",
    "    featurevec.append(np.mean(segment['data']))\n",
    "    featurevec.append(np.min(segment['data']))\n",
    "    featurevec.append(np.percentile(segment['data'],10))\n",
    "    featurevec.append(np.median(segment['data']))\n",
    "    featurevec.append(np.percentile(segment['data'],90))\n",
    "    featurevec.append(np.max(segment['data']))\n",
    "    featurevec.append(np.std(segment['data']))\n",
    "    featurevec.append(scipy.stats.skew(segment['data']))\n",
    "    featurevec.append(scipy.stats.kurtosis(segment['data']))\n",
    "    \n",
    "    # Spectral features\n",
    "    spectral_power = abs(np.fft.fft(segment['data']))**2\n",
    "    window = np.hanning(51)\n",
    "    spectral_power_filt = np.convolve(window/window.sum(),spectral_power,mode='same')\n",
    "    for i in range(1,10):\n",
    "        featurevec.append(np.mean(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        featurevec.append(np.max(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        \n",
    "    peaks = signal.find_peaks(spectral_power_filt[1:15000],distance=1000,height=1)\n",
    "    top3peaks = peaks[0][np.argsort(peaks[1]['peak_heights'])[-3:]]\n",
    "    top3peaks = np.sort(top3peaks)\n",
    "    for i in range(3):\n",
    "        featurevec.append(top3peaks[i])\n",
    "        featurevec.append(spectral_power_filt[top3peaks[i]])\n",
    "    \n",
    "    features['featurevec'] = featurevec\n",
    "    features['time_to_failure'] = segment['time_to_failure']\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/features/features_%04d.pkl'\n",
    "              %segment_index, 'wb') as f:\n",
    "        pickle.dump(features,f)\n",
    "        \n",
    "    featuremat[segment_index,:] = featurevec\n",
    "    time_to_failure_vec[segment_index] = segment['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/training_data/featurematrix',featuremat)\n",
    "np.save('data/training_data/time_to_failure',time_to_failure_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test segments and calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfiles = os.listdir('data/test/')\n",
    "filenames = []\n",
    "for file in testfiles:\n",
    "    filename,_ = os.path.splitext(file)\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testmat = np.zeros([len(testfiles),33])\n",
    "\n",
    "for testfile_index in range(len(testfiles)):\n",
    "    featurevec = []\n",
    "    testsegment = pd.read_csv(os.path.join('data/test/',testfiles[testfile_index]))\n",
    "    testsegment = testsegment['acoustic_data']\n",
    "    \n",
    "    featurevec.append(np.mean(testsegment))\n",
    "    featurevec.append(np.min(testsegment))\n",
    "    featurevec.append(np.percentile(testsegment,10))\n",
    "    featurevec.append(np.median(testsegment))\n",
    "    featurevec.append(np.percentile(testsegment,90))\n",
    "    featurevec.append(np.max(testsegment))\n",
    "    featurevec.append(np.std(testsegment))\n",
    "    featurevec.append(scipy.stats.skew(testsegment))\n",
    "    featurevec.append(scipy.stats.kurtosis(testsegment))\n",
    "    \n",
    "    # Spectral power features\n",
    "    spectral_power = abs(np.fft.fft(testsegment))**2\n",
    "    window = np.hanning(51)\n",
    "    spectral_power_filt = np.convolve(window/window.sum(),spectral_power,mode='same')\n",
    "    for i in range(1,10):\n",
    "        featurevec.append(np.mean(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        featurevec.append(np.max(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        \n",
    "    peaks = signal.find_peaks(spectral_power_filt[1:15000],distance=1000,height=1)\n",
    "    top3peaks = peaks[0][np.argsort(peaks[1]['peak_heights'])[-3:]]\n",
    "    top3peaks = np.sort(top3peaks)\n",
    "    for i in range(3):\n",
    "        featurevec.append(top3peaks[i])\n",
    "        featurevec.append(spectral_power_filt[top3peaks[i]])\n",
    "    \n",
    "    \n",
    "    testmat[testfile_index,:] = featurevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/test_processed/featurematrix_test',testmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test features and time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuremat = np.load('data/training_data/featurematrix.npy')\n",
    "time_to_failure_vec = np.load('data/training_data/time_to_failure.npy')\n",
    "testmat = np.load('data/test_processed/featurematrix_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = np.random.choice(np.arange(len(time_to_failure_vec)),size=int(len(time_to_failure_vec)*.9),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationset = list(set(np.arange(len(time_to_failure_vec)))-set(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat = featuremat[trainset,:]\n",
    "validationmat = featuremat[validationset,:]\n",
    "\n",
    "time_to_failure_train = time_to_failure_vec[trainset]\n",
    "time_to_failure_validation = time_to_failure_vec[validationset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat_scale = scaler.transform(trainmat)\n",
    "validationmat_scale = scaler.transform(validationmat)\n",
    "testmat_scale = scaler.transform(testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(predictions,real_time):\n",
    "    return(np.mean(np.abs(np.squeeze(real_time)-np.squeeze(predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_use = list(range(0,27)) + list([27,29,31])\n",
    "features_use = [1]\n",
    "\n",
    "trainmat_use = trainmat_scale[:,features_use]\n",
    "validationmat_use = validationmat_scale[:,features_use]\n",
    "testmat_use = testmat_scale[:,features_use]\n",
    "\n",
    "num_features = len(features_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in features_use:\n",
    "    print(featureselection[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(trainmat_use, time_to_failure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_train = reg.predict(trainmat_use)\n",
    "predict_train[predict_train<0] = 0\n",
    "\n",
    "predict_validation = reg.predict(validationmat_use)\n",
    "predict_validation[predict_validation<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(get_error(predict_train,time_to_failure_train))\n",
    "print(get_error(predict_validation,time_to_failure_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time_to_failure_train[::10],predict_train[::10],'.')\n",
    "plt.plot(time_to_failure_validation,predict_validation,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = reg.predict(testmat_scale)\n",
    "predict_test[predict_test<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame({'seg_id':filenames,'time_to_failure':predict_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict.to_csv('data/predictions_linear_reg_positive.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "targets = tf.placeholder(tf.float32, shape=(None))\n",
    "lr = tf.placeholder(tf.float32, shape=(None))\n",
    "is_training = tf.placeholder(tf.bool, shape=(None))\n",
    "\n",
    "\n",
    "layer1 = tf.layers.dense(inputs = inputs,\n",
    "                         units = 32,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True,\n",
    "                         )\n",
    "layer1 = tf.layers.dropout(inputs=layer1,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "layer2 = tf.layers.dense(inputs = layer1,\n",
    "                         units = 64,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True)\n",
    "layer2 = tf.layers.dropout(inputs=layer2,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "layer3 = tf.layers.dense(inputs = layer2,\n",
    "                         units = 128,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True)\n",
    "layer3 = tf.layers.dropout(inputs=layer3,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "output = tf.layers.dense(inputs = layer3,\n",
    "                         units = 1,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = False)\n",
    "predictions = tf.reshape(output, [-1])\n",
    "\n",
    "# loss = tf.losses.mean_squared_error(labels=targets,\n",
    "#                                     predictions=predictions)\n",
    "loss = tf.losses.absolute_difference(labels=targets,\n",
    "                                     predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                             epsilon=.1)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_initial = .1\n",
    "lr_decay = 100\n",
    "num_epochs = 1000\n",
    "batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_points = len(time_to_failure_train)\n",
    "batches_per_epoch = int(train_points/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "l_step = []\n",
    "error_train = np.zeros(num_epochs)\n",
    "error_validation = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(batches_per_epoch):\n",
    "        choice = np.arange(train_points)\n",
    "        batch = np.random.choice(choice,batchsize,replace=False)\n",
    "        choice = np.array(set(choice) - set(batch))\n",
    "        _, l = sess.run([train_op,loss],feed_dict={inputs: trainmat_use[batch,:],\n",
    "                                                   targets: time_to_failure_train[batch],\n",
    "                                                   lr: lr_initial*.5**(int(epoch/lr_decay)),\n",
    "                                                   is_training: True})\n",
    "        l_step.append(l)\n",
    "    \n",
    "\n",
    "    p_train = sess.run(predictions,feed_dict={inputs: trainmat_use,\n",
    "                                              is_training: False})\n",
    "    p_validation = sess.run(predictions,feed_dict={inputs: validationmat_use,\n",
    "                                                   is_training: False})\n",
    "    \n",
    "    error_train[epoch] = np.mean(np.abs(p_train-time_to_failure_train))\n",
    "    error_validation[epoch] = np.mean(np.abs(p_validation-time_to_failure_validation))\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print('epoch %i, trainerror %.3f, validationerror %.3f' \n",
    "              %(epoch, error_train[epoch], error_validation[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(error_train)\n",
    "plt.plot(error_validation)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_train = sess.run(predictions,feed_dict={inputs: trainmat_use,is_training:False})\n",
    "predictions_validation = sess.run(predictions,feed_dict={inputs: validationmat_use,is_training:False})\n",
    "\n",
    "plt.plot(time_to_failure_train[0::10],predictions_train[0::10],'.')\n",
    "plt.plot(time_to_failure_validation,predictions_validation,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_test = np.float64(sess.run(predictions,feed_dict={inputs: testmat_scale}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict_tf = pd.DataFrame({'seg_id':filenames,'time_to_failure':prediction_test})\n",
    "df_predict_tf.to_csv('data/predictions_5layer_tf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1, \n",
    "                input_dim=num_features, \n",
    "#                 kernel_initializer='normal', \n",
    "                activation='relu',\n",
    "               ))\n",
    "\n",
    "# model.add(Dense(units=64,\n",
    "# #                 kernel_initializer='normal', \n",
    "#                 activation='relu',\n",
    "#                ))\n",
    "\n",
    "# model.add(Dense(units=128,\n",
    "# #                 kernel_initializer='normal', \n",
    "#                 activation='relu',\n",
    "#                ))\n",
    "\n",
    "# model.add(Dense(1,\n",
    "#                 activation='relu',\n",
    "#                 use_bias=False,\n",
    "# #                 kernel_initializer='normal',\n",
    "#                ))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.1,decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_Callback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch%50==0:\n",
    "            print('Epoch %d, loss %.3f' %(epoch,logs.get('loss')))\n",
    "        self.losses.append(logs.get('val_loss'))\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "    \n",
    "my_callback = My_Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist = model.fit(trainmat_use, \n",
    "          time_to_failure_train, \n",
    "          validation_data=(validationmat_use,time_to_failure_validation), \n",
    "          epochs=100, \n",
    "          batch_size=64,\n",
    "#           callbacks=[my_callback],\n",
    "#           verbose=0\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_predict_train = model.predict(trainmat_use)\n",
    "DNN_predict_validation = model.predict(validationmat_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time_to_failure_train[::10],DNN_predict_train[::10],'.')\n",
    "plt.plot(time_to_failure_validation,DNN_predict_validation,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_error(predictions=DNN_predict_train[::20],real_time=time_to_failure_train[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_error(predictions=DNN_predict_validation,real_time=time_to_failure_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

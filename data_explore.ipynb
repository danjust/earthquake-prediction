{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into 150,000 point segments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_segments = int(len(data['acoustic_data'])/150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segment = {}\n",
    "\n",
    "ind_save = 0\n",
    "for i in range(num_segments):\n",
    "    if data['time_to_failure'].values[i*150000] > data['time_to_failure'].values[(i+1)*150000-1]:\n",
    "        segment['data'] = data['acoustic_data'].values[i*150000:(i+1)*150000]\n",
    "        segment['time_to_failure'] = data['time_to_failure'].values[(i+1)*150000]\n",
    "        with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'%ind_save, 'wb') as f:\n",
    "            pickle.dump(segment,f)\n",
    "        ind_save = ind_save+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_segments_use = ind_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spectrograms for segments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = 4000000\n",
    "sampling_period = 1 / fs\n",
    "\n",
    "spectrum = {}\n",
    "\n",
    "for i in range(num_segments_use):\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'%i, 'rb') as f:\n",
    "        segment = pickle.load(f)\n",
    "    _,_, s = signal.spectrogram(segment['data'], fs, window=('tukey', .25))\n",
    "    spectrum['data'] = s[:33,:]\n",
    "    spectrum['time_to_failure'] = segment['time_to_failure']\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/spectrograms/spectrum_%04d.pkl'%i, 'wb') as f:\n",
    "        pickle.dump(spectrum,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features for segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureselection = ['mean', 'min', '10th percentile', 'median', '90th percentile', 'max', \n",
    "                    'stdev', 'skewness', 'kurtosis',\n",
    "                    'mean spectral power 1500-2999', 'max spectral power 1500-2999',\n",
    "                    'mean spectral power 3000-4499', 'max spectral power 3000-4499',\n",
    "                    'mean spectral power 4500-5999', 'max spectral power 4500-5999',\n",
    "                    'mean spectral power 6000-7499', 'max spectral power 6000-7499',\n",
    "                    'mean spectral power 7500-8999', 'max spectral power 7500-8999',\n",
    "                    'mean spectral power 9000-10499', 'max spectral power 9000-10499',\n",
    "                    'mean spectral power 10500-11999', 'max spectral power 10500-11999',\n",
    "                    'mean spectral power 12000-13499', 'max spectral power 12000-13499',\n",
    "                    'mean spectral power 13500-15000', 'max spectral power 13500-15000',\n",
    "                    'location of 1st of 3 highest peaks', 'amplitude of 1st of 3 highest peaks',\n",
    "                    'location of 2nd of 3 highest peaks', 'amplitude of 2nd of 3 highest peaks',\n",
    "                    'location of 3rd of 3 highest peaks', 'amplitude of 3rd of 3 highest peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "featuremat = np.zeros([num_segments_use,33])\n",
    "time_to_failure_vec = np.zeros(num_segments_use)\n",
    "\n",
    "for segment_index in range(num_segments_use):\n",
    "    featurevec = []\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/segments/segment_%04d.pkl'\n",
    "              %segment_index, 'rb') as f:\n",
    "        segment = pickle.load(f)\n",
    "    featurevec.append(np.mean(segment['data']))\n",
    "    featurevec.append(np.min(segment['data']))\n",
    "    featurevec.append(np.percentile(segment['data'],10))\n",
    "    featurevec.append(np.median(segment['data']))\n",
    "    featurevec.append(np.percentile(segment['data'],90))\n",
    "    featurevec.append(np.max(segment['data']))\n",
    "    featurevec.append(np.std(segment['data']))\n",
    "    featurevec.append(scipy.stats.skew(segment['data']))\n",
    "    featurevec.append(scipy.stats.kurtosis(segment['data']))\n",
    "    \n",
    "    # Spectral features\n",
    "    spectral_power = abs(np.fft.fft(segment['data']))**2\n",
    "    window = np.hanning(51)\n",
    "    spectral_power_filt = np.convolve(window/window.sum(),spectral_power,mode='same')\n",
    "    for i in range(1,10):\n",
    "        featurevec.append(np.mean(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        featurevec.append(np.max(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        \n",
    "    peaks = signal.find_peaks(spectral_power_filt[1:15000],distance=1000,height=1)\n",
    "    top3peaks = peaks[0][np.argsort(peaks[1]['peak_heights'])[-3:]]\n",
    "    top3peaks = np.sort(top3peaks)\n",
    "    for i in range(3):\n",
    "        featurevec.append(top3peaks[i])\n",
    "        featurevec.append(spectral_power_filt[top3peaks[i]])\n",
    "    \n",
    "    features['featurevec'] = featurevec\n",
    "    features['time_to_failure'] = segment['time_to_failure']\n",
    "    with open('/Users/djustus/workspace/earthquake/data/training_data/features/features_%04d.pkl'\n",
    "              %segment_index, 'wb') as f:\n",
    "        pickle.dump(features,f)\n",
    "        \n",
    "    featuremat[segment_index,:] = featurevec\n",
    "    time_to_failure_vec[segment_index] = segment['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/training_data/featurematrix',featuremat)\n",
    "np.save('data/training_data/time_to_failure',time_to_failure_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test segments and calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfiles = os.listdir('data/test/')\n",
    "filenames = []\n",
    "for file in testfiles:\n",
    "    filename,_ = os.path.splitext(file)\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testmat = np.zeros([len(testfiles),33])\n",
    "\n",
    "for testfile_index in range(len(testfiles)):\n",
    "    featurevec = []\n",
    "    testsegment = pd.read_csv(os.path.join('data/test/',testfiles[testfile_index]))\n",
    "    testsegment = testsegment['acoustic_data']\n",
    "    \n",
    "    featurevec.append(np.mean(testsegment))\n",
    "    featurevec.append(np.min(testsegment))\n",
    "    featurevec.append(np.percentile(testsegment,10))\n",
    "    featurevec.append(np.median(testsegment))\n",
    "    featurevec.append(np.percentile(testsegment,90))\n",
    "    featurevec.append(np.max(testsegment))\n",
    "    featurevec.append(np.std(testsegment))\n",
    "    featurevec.append(scipy.stats.skew(testsegment))\n",
    "    featurevec.append(scipy.stats.kurtosis(testsegment))\n",
    "    \n",
    "    # Spectral power features\n",
    "    spectral_power = abs(np.fft.fft(testsegment))**2\n",
    "    window = np.hanning(51)\n",
    "    spectral_power_filt = np.convolve(window/window.sum(),spectral_power,mode='same')\n",
    "    for i in range(1,10):\n",
    "        featurevec.append(np.mean(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        featurevec.append(np.max(spectral_power_filt[i*1500:(i+1)*1500]))\n",
    "        \n",
    "    peaks = signal.find_peaks(spectral_power_filt[1:15000],distance=1000,height=1)\n",
    "    top3peaks = peaks[0][np.argsort(peaks[1]['peak_heights'])[-3:]]\n",
    "    top3peaks = np.sort(top3peaks)\n",
    "    for i in range(3):\n",
    "        featurevec.append(top3peaks[i])\n",
    "        featurevec.append(spectral_power_filt[top3peaks[i]])\n",
    "    \n",
    "    \n",
    "    testmat[testfile_index,:] = featurevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/test_processed/featurematrix_test',testmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test features and time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuremat = np.load('data/training_data/featurematrix.npy')\n",
    "time_to_failure_vec = np.load('data/training_data/time_to_failure.npy')\n",
    "testmat = np.load('data/test_processed/featurematrix_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = np.random.choice(np.arange(len(time_to_failure_vec)),size=int(len(time_to_failure_vec)*.9),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationset = list(set(np.arange(len(time_to_failure_vec)))-set(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat = featuremat[trainset,:]\n",
    "validationmat = featuremat[validationset,:]\n",
    "\n",
    "time_to_failure_train = time_to_failure_vec[trainset]\n",
    "time_to_failure_validation = time_to_failure_vec[validationset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat_scale = scaler.transform(trainmat)\n",
    "validationmat_scale = scaler.transform(validationmat)\n",
    "testmat_scale = scaler.transform(testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(predictions,real_time):\n",
    "    return(np.mean(np.abs(np.squeeze(real_time)-np.squeeze(predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_use = list(range(0,27)) + list([27,29,31])\n",
    "features_use = [1]\n",
    "\n",
    "trainmat_use = trainmat_scale[:,features_use]\n",
    "validationmat_use = validationmat_scale[:,features_use]\n",
    "testmat_use = testmat_scale[:,features_use]\n",
    "\n",
    "num_features = len(features_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n"
     ]
    }
   ],
   "source": [
    "for f in features_use:\n",
    "    print(featureselection[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(trainmat_use, time_to_failure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_train = reg.predict(trainmat_use)\n",
    "predict_train[predict_train<0] = 0\n",
    "\n",
    "predict_validation = reg.predict(validationmat_use)\n",
    "predict_validation[predict_validation<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_error(predict_train,time_to_failure_train))\n",
    "print(get_error(predict_validation,time_to_failure_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_to_failure_train[::10],predict_train[::10],'.')\n",
    "plt.plot(time_to_failure_validation,predict_validation,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = reg.predict(testmat_scale)\n",
    "predict_test[predict_test<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame({'seg_id':filenames,'time_to_failure':predict_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict.to_csv('data/predictions_linear_reg_positive.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "targets = tf.placeholder(tf.float32, shape=(None))\n",
    "lr = tf.placeholder(tf.float32, shape=(None))\n",
    "is_training = tf.placeholder(tf.bool, shape=(None))\n",
    "\n",
    "\n",
    "layer1 = tf.layers.dense(inputs = inputs,\n",
    "                         units = 32,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True,\n",
    "                         )\n",
    "layer1 = tf.layers.dropout(inputs=layer1,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "layer2 = tf.layers.dense(inputs = layer1,\n",
    "                         units = 64,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True)\n",
    "layer2 = tf.layers.dropout(inputs=layer2,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "layer3 = tf.layers.dense(inputs = layer2,\n",
    "                         units = 128,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = True)\n",
    "layer3 = tf.layers.dropout(inputs=layer3,\n",
    "                           rate=dropout,\n",
    "                           training=is_training)\n",
    "\n",
    "output = tf.layers.dense(inputs = layer3,\n",
    "                         units = 1,\n",
    "                         activation = tf.nn.relu,   \n",
    "                         use_bias = False)\n",
    "predictions = tf.reshape(output, [-1])\n",
    "\n",
    "# loss = tf.losses.mean_squared_error(labels=targets,\n",
    "#                                     predictions=predictions)\n",
    "loss = tf.losses.absolute_difference(labels=targets,\n",
    "                                     predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                             epsilon=.1)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_initial = .1\n",
    "lr_decay = 100\n",
    "num_epochs = 1000\n",
    "batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_points = len(time_to_failure_train)\n",
    "batches_per_epoch = int(train_points/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "l_step = []\n",
    "error_train = np.zeros(num_epochs)\n",
    "error_validation = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(batches_per_epoch):\n",
    "        choice = np.arange(train_points)\n",
    "        batch = np.random.choice(choice,batchsize,replace=False)\n",
    "        choice = np.array(set(choice) - set(batch))\n",
    "        _, l = sess.run([train_op,loss],feed_dict={inputs: trainmat_use[batch,:],\n",
    "                                                   targets: time_to_failure_train[batch],\n",
    "                                                   lr: lr_initial*.5**(int(epoch/lr_decay)),\n",
    "                                                   is_training: True})\n",
    "        l_step.append(l)\n",
    "    \n",
    "\n",
    "    p_train = sess.run(predictions,feed_dict={inputs: trainmat_use,\n",
    "                                              is_training: False})\n",
    "    p_validation = sess.run(predictions,feed_dict={inputs: validationmat_use,\n",
    "                                                   is_training: False})\n",
    "    \n",
    "    error_train[epoch] = np.mean(np.abs(p_train-time_to_failure_train))\n",
    "    error_validation[epoch] = np.mean(np.abs(p_validation-time_to_failure_validation))\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print('epoch %i, trainerror %.3f, validationerror %.3f' \n",
    "              %(epoch, error_train[epoch], error_validation[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(error_train)\n",
    "plt.plot(error_validation)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = sess.run(predictions,feed_dict={inputs: trainmat_use,is_training:False})\n",
    "predictions_validation = sess.run(predictions,feed_dict={inputs: validationmat_use,is_training:False})\n",
    "\n",
    "plt.plot(time_to_failure_train[0::10],predictions_train[0::10],'.')\n",
    "plt.plot(time_to_failure_validation,predictions_validation,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_test = np.float64(sess.run(predictions,feed_dict={inputs: testmat_scale}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict_tf = pd.DataFrame({'seg_id':filenames,'time_to_failure':prediction_test})\n",
    "df_predict_tf.to_csv('data/predictions_5layer_tf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1, \n",
    "                input_dim=num_features, \n",
    "#                 kernel_initializer='normal', \n",
    "                activation='relu',\n",
    "               ))\n",
    "\n",
    "# model.add(Dense(units=64,\n",
    "# #                 kernel_initializer='normal', \n",
    "#                 activation='relu',\n",
    "#                ))\n",
    "\n",
    "# model.add(Dense(units=128,\n",
    "# #                 kernel_initializer='normal', \n",
    "#                 activation='relu',\n",
    "#                ))\n",
    "\n",
    "# model.add(Dense(1,\n",
    "#                 activation='relu',\n",
    "#                 use_bias=False,\n",
    "# #                 kernel_initializer='normal',\n",
    "#                ))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.1,decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_Callback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch%50==0:\n",
    "            print('Epoch %d, loss %.3f' %(epoch,logs.get('loss')))\n",
    "        self.losses.append(logs.get('val_loss'))\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "    \n",
    "my_callback = My_Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3760 samples, validate on 418 samples\n",
      "Epoch 1/100\n",
      "3760/3760 [==============================] - 0s 84us/step - loss: 35.2369 - mean_absolute_error: 4.7052 - val_loss: 30.8689 - val_mean_absolute_error: 4.4389\n",
      "Epoch 2/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 25.4535 - mean_absolute_error: 3.9362 - val_loss: 26.0931 - val_mean_absolute_error: 4.0682\n",
      "Epoch 3/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 22.2974 - mean_absolute_error: 3.6746 - val_loss: 23.7127 - val_mean_absolute_error: 3.8720\n",
      "Epoch 4/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 20.5500 - mean_absolute_error: 3.5268 - val_loss: 22.2308 - val_mean_absolute_error: 3.7497\n",
      "Epoch 5/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 19.4039 - mean_absolute_error: 3.4268 - val_loss: 21.1832 - val_mean_absolute_error: 3.6622\n",
      "Epoch 6/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 18.5734 - mean_absolute_error: 3.3549 - val_loss: 20.3924 - val_mean_absolute_error: 3.5950\n",
      "Epoch 7/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 17.9371 - mean_absolute_error: 3.3016 - val_loss: 19.7636 - val_mean_absolute_error: 3.5414\n",
      "Epoch 8/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 17.4272 - mean_absolute_error: 3.2585 - val_loss: 19.2384 - val_mean_absolute_error: 3.4967\n",
      "Epoch 9/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 17.0030 - mean_absolute_error: 3.2235 - val_loss: 18.7998 - val_mean_absolute_error: 3.4590\n",
      "Epoch 10/100\n",
      "3760/3760 [==============================] - ETA: 0s - loss: 16.8295 - mean_absolute_error: 3.21 - 0s 18us/step - loss: 16.6452 - mean_absolute_error: 3.1946 - val_loss: 18.4248 - val_mean_absolute_error: 3.4268\n",
      "Epoch 11/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 16.3367 - mean_absolute_error: 3.1696 - val_loss: 18.1000 - val_mean_absolute_error: 3.3984\n",
      "Epoch 12/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 16.0679 - mean_absolute_error: 3.1478 - val_loss: 17.8119 - val_mean_absolute_error: 3.3731\n",
      "Epoch 13/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 15.8302 - mean_absolute_error: 3.1282 - val_loss: 17.5553 - val_mean_absolute_error: 3.3507\n",
      "Epoch 14/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 15.6179 - mean_absolute_error: 3.1111 - val_loss: 17.3268 - val_mean_absolute_error: 3.3306\n",
      "Epoch 15/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 15.4275 - mean_absolute_error: 3.0954 - val_loss: 17.1183 - val_mean_absolute_error: 3.3122\n",
      "Epoch 16/100\n",
      "3760/3760 [==============================] - 0s 19us/step - loss: 15.2548 - mean_absolute_error: 3.0818 - val_loss: 16.9310 - val_mean_absolute_error: 3.2958\n",
      "Epoch 17/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 15.0988 - mean_absolute_error: 3.0692 - val_loss: 16.7565 - val_mean_absolute_error: 3.2804\n",
      "Epoch 18/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 14.9550 - mean_absolute_error: 3.0577 - val_loss: 16.5969 - val_mean_absolute_error: 3.2665\n",
      "Epoch 19/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 14.8227 - mean_absolute_error: 3.0471 - val_loss: 16.4527 - val_mean_absolute_error: 3.2539\n",
      "Epoch 20/100\n",
      "3760/3760 [==============================] - 0s 19us/step - loss: 14.7010 - mean_absolute_error: 3.0372 - val_loss: 16.3159 - val_mean_absolute_error: 3.2420\n",
      "Epoch 21/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 14.5870 - mean_absolute_error: 3.0283 - val_loss: 16.1868 - val_mean_absolute_error: 3.2306\n",
      "Epoch 22/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 14.4800 - mean_absolute_error: 3.0195 - val_loss: 16.0693 - val_mean_absolute_error: 3.2203\n",
      "Epoch 23/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 14.3811 - mean_absolute_error: 3.0119 - val_loss: 15.9573 - val_mean_absolute_error: 3.2103\n",
      "Epoch 24/100\n",
      "3760/3760 [==============================] - 0s 11us/step - loss: 14.2880 - mean_absolute_error: 3.0043 - val_loss: 15.8540 - val_mean_absolute_error: 3.2009\n",
      "Epoch 25/100\n",
      "3760/3760 [==============================] - 0s 10us/step - loss: 14.2010 - mean_absolute_error: 2.9974 - val_loss: 15.7558 - val_mean_absolute_error: 3.1920\n",
      "Epoch 26/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 14.1192 - mean_absolute_error: 2.9908 - val_loss: 15.6631 - val_mean_absolute_error: 3.1838\n",
      "Epoch 27/100\n",
      "3760/3760 [==============================] - 0s 11us/step - loss: 14.0419 - mean_absolute_error: 2.9847 - val_loss: 15.5778 - val_mean_absolute_error: 3.1763\n",
      "Epoch 28/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.9693 - mean_absolute_error: 2.9786 - val_loss: 15.4952 - val_mean_absolute_error: 3.1689\n",
      "Epoch 29/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 13.9010 - mean_absolute_error: 2.9730 - val_loss: 15.4156 - val_mean_absolute_error: 3.1619\n",
      "Epoch 30/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.8356 - mean_absolute_error: 2.9679 - val_loss: 15.3421 - val_mean_absolute_error: 3.1553\n",
      "Epoch 31/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.7736 - mean_absolute_error: 2.9628 - val_loss: 15.2710 - val_mean_absolute_error: 3.1489\n",
      "Epoch 32/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 13.7146 - mean_absolute_error: 2.9579 - val_loss: 15.2032 - val_mean_absolute_error: 3.1426\n",
      "Epoch 33/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.6584 - mean_absolute_error: 2.9534 - val_loss: 15.1395 - val_mean_absolute_error: 3.1368\n",
      "Epoch 34/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.6051 - mean_absolute_error: 2.9492 - val_loss: 15.0788 - val_mean_absolute_error: 3.1313\n",
      "Epoch 35/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.5543 - mean_absolute_error: 2.9453 - val_loss: 15.0203 - val_mean_absolute_error: 3.1263\n",
      "Epoch 36/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.5055 - mean_absolute_error: 2.9413 - val_loss: 14.9652 - val_mean_absolute_error: 3.1217\n",
      "Epoch 37/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.4590 - mean_absolute_error: 2.9377 - val_loss: 14.9099 - val_mean_absolute_error: 3.1171\n",
      "Epoch 38/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 13.4139 - mean_absolute_error: 2.9341 - val_loss: 14.8596 - val_mean_absolute_error: 3.1129\n",
      "Epoch 39/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 13.3713 - mean_absolute_error: 2.9308 - val_loss: 14.8097 - val_mean_absolute_error: 3.1088\n",
      "Epoch 40/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.3303 - mean_absolute_error: 2.9278 - val_loss: 14.7614 - val_mean_absolute_error: 3.1048\n",
      "Epoch 41/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 13.2909 - mean_absolute_error: 2.9247 - val_loss: 14.7163 - val_mean_absolute_error: 3.1010\n",
      "Epoch 42/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 13.2532 - mean_absolute_error: 2.9218 - val_loss: 14.6736 - val_mean_absolute_error: 3.0973\n",
      "Epoch 43/100\n",
      "3760/3760 [==============================] - ETA: 0s - loss: 13.1273 - mean_absolute_error: 2.91 - 0s 17us/step - loss: 13.2173 - mean_absolute_error: 2.9191 - val_loss: 14.6310 - val_mean_absolute_error: 3.0937\n",
      "Epoch 44/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 13.1825 - mean_absolute_error: 2.9164 - val_loss: 14.5904 - val_mean_absolute_error: 3.0903\n",
      "Epoch 45/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.1490 - mean_absolute_error: 2.9138 - val_loss: 14.5530 - val_mean_absolute_error: 3.0871\n",
      "Epoch 46/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 13.1174 - mean_absolute_error: 2.9115 - val_loss: 14.5139 - val_mean_absolute_error: 3.0838\n",
      "Epoch 47/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.0859 - mean_absolute_error: 2.9091 - val_loss: 14.4798 - val_mean_absolute_error: 3.0809\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3760/3760 [==============================] - 0s 14us/step - loss: 13.0562 - mean_absolute_error: 2.9068 - val_loss: 14.4436 - val_mean_absolute_error: 3.0778\n",
      "Epoch 49/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 13.0271 - mean_absolute_error: 2.9046 - val_loss: 14.4103 - val_mean_absolute_error: 3.0749\n",
      "Epoch 50/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.9991 - mean_absolute_error: 2.9025 - val_loss: 14.3774 - val_mean_absolute_error: 3.0721\n",
      "Epoch 51/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.9722 - mean_absolute_error: 2.9005 - val_loss: 14.3459 - val_mean_absolute_error: 3.0693\n",
      "Epoch 52/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.9461 - mean_absolute_error: 2.8984 - val_loss: 14.3159 - val_mean_absolute_error: 3.0668\n",
      "Epoch 53/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.9210 - mean_absolute_error: 2.8966 - val_loss: 14.2860 - val_mean_absolute_error: 3.0642\n",
      "Epoch 54/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.8967 - mean_absolute_error: 2.8948 - val_loss: 14.2570 - val_mean_absolute_error: 3.0617\n",
      "Epoch 55/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.8730 - mean_absolute_error: 2.8932 - val_loss: 14.2302 - val_mean_absolute_error: 3.0594\n",
      "Epoch 56/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.8503 - mean_absolute_error: 2.8915 - val_loss: 14.2035 - val_mean_absolute_error: 3.0570\n",
      "Epoch 57/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.8284 - mean_absolute_error: 2.8898 - val_loss: 14.1771 - val_mean_absolute_error: 3.0547\n",
      "Epoch 58/100\n",
      "3760/3760 [==============================] - ETA: 0s - loss: 12.8099 - mean_absolute_error: 2.88 - 0s 16us/step - loss: 12.8071 - mean_absolute_error: 2.8883 - val_loss: 14.1516 - val_mean_absolute_error: 3.0524\n",
      "Epoch 59/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.7863 - mean_absolute_error: 2.8868 - val_loss: 14.1273 - val_mean_absolute_error: 3.0501\n",
      "Epoch 60/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.7665 - mean_absolute_error: 2.8854 - val_loss: 14.1027 - val_mean_absolute_error: 3.0478\n",
      "Epoch 61/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 12.7469 - mean_absolute_error: 2.8841 - val_loss: 14.0805 - val_mean_absolute_error: 3.0458\n",
      "Epoch 62/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.7281 - mean_absolute_error: 2.8827 - val_loss: 14.0583 - val_mean_absolute_error: 3.0437\n",
      "Epoch 63/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.7099 - mean_absolute_error: 2.8814 - val_loss: 14.0367 - val_mean_absolute_error: 3.0418\n",
      "Epoch 64/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.6921 - mean_absolute_error: 2.8800 - val_loss: 14.0159 - val_mean_absolute_error: 3.0400\n",
      "Epoch 65/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 12.6748 - mean_absolute_error: 2.8788 - val_loss: 13.9959 - val_mean_absolute_error: 3.0383\n",
      "Epoch 66/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.6581 - mean_absolute_error: 2.8777 - val_loss: 13.9755 - val_mean_absolute_error: 3.0366\n",
      "Epoch 67/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 12.6419 - mean_absolute_error: 2.8765 - val_loss: 13.9559 - val_mean_absolute_error: 3.0349\n",
      "Epoch 68/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.6261 - mean_absolute_error: 2.8753 - val_loss: 13.9378 - val_mean_absolute_error: 3.0333\n",
      "Epoch 69/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 12.6110 - mean_absolute_error: 2.8743 - val_loss: 13.9191 - val_mean_absolute_error: 3.0316\n",
      "Epoch 70/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.5960 - mean_absolute_error: 2.8732 - val_loss: 13.9019 - val_mean_absolute_error: 3.0301\n",
      "Epoch 71/100\n",
      "3760/3760 [==============================] - 0s 18us/step - loss: 12.5817 - mean_absolute_error: 2.8722 - val_loss: 13.8844 - val_mean_absolute_error: 3.0287\n",
      "Epoch 72/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.5677 - mean_absolute_error: 2.8712 - val_loss: 13.8680 - val_mean_absolute_error: 3.0273\n",
      "Epoch 73/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.5540 - mean_absolute_error: 2.8702 - val_loss: 13.8525 - val_mean_absolute_error: 3.0261\n",
      "Epoch 74/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.5411 - mean_absolute_error: 2.8694 - val_loss: 13.8358 - val_mean_absolute_error: 3.0248\n",
      "Epoch 75/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.5281 - mean_absolute_error: 2.8684 - val_loss: 13.8206 - val_mean_absolute_error: 3.0236\n",
      "Epoch 76/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.5156 - mean_absolute_error: 2.8675 - val_loss: 13.8058 - val_mean_absolute_error: 3.0224\n",
      "Epoch 77/100\n",
      "3760/3760 [==============================] - 0s 17us/step - loss: 12.5035 - mean_absolute_error: 2.8666 - val_loss: 13.7914 - val_mean_absolute_error: 3.0213\n",
      "Epoch 78/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.4918 - mean_absolute_error: 2.8658 - val_loss: 13.7773 - val_mean_absolute_error: 3.0202\n",
      "Epoch 79/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 12.4804 - mean_absolute_error: 2.8650 - val_loss: 13.7635 - val_mean_absolute_error: 3.0191\n",
      "Epoch 80/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.4692 - mean_absolute_error: 2.8642 - val_loss: 13.7498 - val_mean_absolute_error: 3.0180\n",
      "Epoch 81/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.4583 - mean_absolute_error: 2.8634 - val_loss: 13.7368 - val_mean_absolute_error: 3.0170\n",
      "Epoch 82/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.4479 - mean_absolute_error: 2.8627 - val_loss: 13.7241 - val_mean_absolute_error: 3.0160\n",
      "Epoch 83/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.4376 - mean_absolute_error: 2.8619 - val_loss: 13.7119 - val_mean_absolute_error: 3.0151\n",
      "Epoch 84/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 12.4275 - mean_absolute_error: 2.8612 - val_loss: 13.6998 - val_mean_absolute_error: 3.0141\n",
      "Epoch 85/100\n",
      "3760/3760 [==============================] - 0s 14us/step - loss: 12.4178 - mean_absolute_error: 2.8605 - val_loss: 13.6879 - val_mean_absolute_error: 3.0132\n",
      "Epoch 86/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 12.4083 - mean_absolute_error: 2.8598 - val_loss: 13.6765 - val_mean_absolute_error: 3.0123\n",
      "Epoch 87/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 12.3989 - mean_absolute_error: 2.8591 - val_loss: 13.6654 - val_mean_absolute_error: 3.0115\n",
      "Epoch 88/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 12.3898 - mean_absolute_error: 2.8585 - val_loss: 13.6550 - val_mean_absolute_error: 3.0107\n",
      "Epoch 89/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 12.3810 - mean_absolute_error: 2.8578 - val_loss: 13.6442 - val_mean_absolute_error: 3.0098\n",
      "Epoch 90/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 12.3724 - mean_absolute_error: 2.8572 - val_loss: 13.6334 - val_mean_absolute_error: 3.0090\n",
      "Epoch 91/100\n",
      "3760/3760 [==============================] - 0s 12us/step - loss: 12.3640 - mean_absolute_error: 2.8566 - val_loss: 13.6238 - val_mean_absolute_error: 3.0083\n",
      "Epoch 92/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 12.3558 - mean_absolute_error: 2.8560 - val_loss: 13.6141 - val_mean_absolute_error: 3.0076\n",
      "Epoch 93/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.3479 - mean_absolute_error: 2.8555 - val_loss: 13.6035 - val_mean_absolute_error: 3.0068\n",
      "Epoch 94/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.3396 - mean_absolute_error: 2.8548 - val_loss: 13.5949 - val_mean_absolute_error: 3.0061\n",
      "Epoch 95/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.3319 - mean_absolute_error: 2.8543 - val_loss: 13.5858 - val_mean_absolute_error: 3.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.3244 - mean_absolute_error: 2.8538 - val_loss: 13.5763 - val_mean_absolute_error: 3.0047\n",
      "Epoch 97/100\n",
      "3760/3760 [==============================] - 0s 13us/step - loss: 12.3168 - mean_absolute_error: 2.8532 - val_loss: 13.5684 - val_mean_absolute_error: 3.0040\n",
      "Epoch 98/100\n",
      "3760/3760 [==============================] - 0s 15us/step - loss: 12.3099 - mean_absolute_error: 2.8527 - val_loss: 13.5587 - val_mean_absolute_error: 3.0033\n",
      "Epoch 99/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.3026 - mean_absolute_error: 2.8522 - val_loss: 13.5510 - val_mean_absolute_error: 3.0027\n",
      "Epoch 100/100\n",
      "3760/3760 [==============================] - 0s 16us/step - loss: 12.2957 - mean_absolute_error: 2.8516 - val_loss: 13.5431 - val_mean_absolute_error: 3.0020\n",
      "CPU times: user 8.97 s, sys: 2.82 s, total: 11.8 s\n",
      "Wall time: 6.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hist = model.fit(trainmat_use, \n",
    "          time_to_failure_train, \n",
    "          validation_data=(validationmat_use,time_to_failure_validation), \n",
    "          epochs=100, \n",
    "          batch_size=64,\n",
    "#           callbacks=[my_callback],\n",
    "#           verbose=0\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_predict_train = model.predict(trainmat_use)\n",
    "DNN_predict_validation = model.predict(validationmat_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_to_failure_train[::10],DNN_predict_train[::10],'.')\n",
    "plt.plot(time_to_failure_validation,DNN_predict_validation,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_error(predictions=DNN_predict_train[::20],real_time=time_to_failure_train[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_error(predictions=DNN_predict_validation,real_time=time_to_failure_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

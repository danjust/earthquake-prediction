{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from evolution import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuremat = np.load('data/training_data/featurematrix.npy')\n",
    "time_to_failure_vec = np.load('data/training_data/time_to_failure.npy')\n",
    "testmat = np.load('data/test_processed/featurematrix_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = np.random.choice(np.arange(len(time_to_failure_vec)),size=int(len(time_to_failure_vec)*.9),replace=False)\n",
    "validationset = list(set(np.arange(len(time_to_failure_vec)))-set(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat = featuremat[trainset,:]\n",
    "validationmat = featuremat[validationset,:]\n",
    "\n",
    "time_to_failure_train = time_to_failure_vec[trainset]\n",
    "time_to_failure_validation = time_to_failure_vec[validationset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainmat_scale = scaler.transform(trainmat)\n",
    "validationmat_scale = scaler.transform(validationmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generations = 100\n",
    "pop_size = 100\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "num_all_features = 33\n",
    "\n",
    "savepath = '/Users/djustus/workspace/earthquake/ckpt/'\n",
    "model_name = 'E1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models with one hidden layer (with 1-16 units) each, using 8 random features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = []\n",
    "for i in range(pop_size):\n",
    "    model_list.append(model(set(np.random.choice(range(num_all_features),1,replace=False)),\n",
    "                            [np.random.randint(1,17)],[0],num_all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_list,trainmat,validationmat,model_ind):\n",
    "    \n",
    "    trainmat_use = trainmat[:,list(model_list[model_ind].featureset)]\n",
    "    validationmat_use = validationmat[:,list(model_list[model_ind].featureset)]\n",
    "\n",
    "    m = model_list[model_ind].build()\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.1,decay=.1)\n",
    "    m.compile(optimizer=opt,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "    hist = m.fit(trainmat_use, \n",
    "          time_to_failure_train, \n",
    "          validation_data=(validationmat_use,time_to_failure_validation), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0\n",
    "         )\n",
    "        \n",
    "    loss = hist.history['val_mean_absolute_error'][-1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train generation 0 completely.  \n",
    "Following generation 1: Train only mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 finished, 145.808 sec,\n",
      "average number of features 1.0, average number of layers 1.0, average loss 3.768, \n",
      "best number of features 1.0, best number of layers 1.0, best loss 2.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djustus/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 finished, 84.714 sec,\n",
      "average number of features 1.1, average number of layers 1.1, average loss 3.252, \n",
      "best number of features 1.0, best number of layers 2.0, best loss 2.230\n",
      "Generation 2 finished, 76.996 sec,\n",
      "average number of features 1.1, average number of layers 1.2, average loss 3.172, \n",
      "best number of features 1.0, best number of layers 2.0, best loss 2.230\n",
      "Generation 3 finished, 84.625 sec,\n",
      "average number of features 1.2, average number of layers 1.2, average loss 3.090, \n",
      "best number of features 1.0, best number of layers 2.0, best loss 2.230\n",
      "Generation 4 finished, 79.382 sec,\n",
      "average number of features 1.4, average number of layers 1.3, average loss 2.739, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.216\n",
      "Generation 5 finished, 89.956 sec,\n",
      "average number of features 1.4, average number of layers 1.4, average loss 2.661, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.216\n",
      "Generation 6 finished, 86.110 sec,\n",
      "average number of features 1.6, average number of layers 1.7, average loss 2.797, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.216\n",
      "Generation 7 finished, 106.142 sec,\n",
      "average number of features 1.6, average number of layers 1.9, average loss 2.647, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.216\n",
      "Generation 8 finished, 94.787 sec,\n",
      "average number of features 1.7, average number of layers 2.2, average loss 2.821, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.215\n",
      "Generation 9 finished, 107.215 sec,\n",
      "average number of features 1.6, average number of layers 2.5, average loss 2.592, \n",
      "best number of features 2.0, best number of layers 3.0, best loss 2.211\n",
      "Generation 10 finished, 100.546 sec,\n",
      "average number of features 1.5, average number of layers 2.5, average loss 2.637, \n",
      "best number of features 2.0, best number of layers 3.0, best loss 2.211\n",
      "Generation 11 finished, 106.557 sec,\n",
      "average number of features 1.6, average number of layers 2.6, average loss 2.743, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.207\n",
      "Generation 12 finished, 104.547 sec,\n",
      "average number of features 1.8, average number of layers 2.7, average loss 2.604, \n",
      "best number of features 2.0, best number of layers 2.0, best loss 2.207\n"
     ]
    }
   ],
   "source": [
    "loss = np.zeros([generations,pop_size])\n",
    "models_employed = []\n",
    "while gen<generations:\n",
    "    gen+=1\n",
    "    num_features_gen = []\n",
    "    num_layers_gen = []\n",
    "    t = time.time()\n",
    "    \n",
    "    models_employed.append(model_list) \n",
    "    \n",
    "    for model_ind in range(pop_size):\n",
    "        num_features_gen.append(model_list[model_ind].num_features)\n",
    "        num_layers_gen.append(model_list[model_ind].num_layers)\n",
    "        \n",
    "    loss_generation = Parallel(n_jobs=num_cores)(\n",
    "            delayed(train_model)(model_list,trainmat_scale,validationmat_scale,model_ind) \n",
    "            for model_ind in range(int(pop_size/2),pop_size)\n",
    "    )\n",
    "#     loss_generation = [train_model(model_list,trainmat_scale,validationmat_scale,model_ind) \n",
    "#                        for model_ind in range(pop_size)]\n",
    "    loss_generation = [1e9 if np.isnan(val) else val for val in loss_generation]\n",
    "    \n",
    "    if gen==0:\n",
    "        loss_survivor = Parallel(n_jobs=num_cores)(\n",
    "            delayed(train_model)(model_list,trainmat_scale,validationmat_scale,model_ind) \n",
    "            for model_ind in range(0,int(pop_size/2))\n",
    "        )\n",
    "        loss_survivor = [1e9 if np.isnan(val) else val for val in loss_survivor]\n",
    "            \n",
    "    loss[gen,0:int(pop_size/2)] = loss_survivor\n",
    "    loss[gen,int(pop_size/2):pop_size] = loss_generation\n",
    "    \n",
    "    tf.keras.backend.clear_session()    \n",
    "        \n",
    "    live = np.argsort(loss[gen,:])[0:int(pop_size/2)]\n",
    "    best = live[0]\n",
    "    \n",
    "    print('Generation %d finished, %.3f sec,\\n'\n",
    "          'average number of features %.1f, average number of layers %.1f, average loss %.3f, \\n'          \n",
    "          'best number of features %.1f, best number of layers %.1f, best loss %.3f'\n",
    "          %(gen, time.time()-t, \n",
    "            np.mean(num_features_gen), np.mean(num_layers_gen), np.mean(loss[gen,:]), \n",
    "            model_list[best].num_features, model_list[best].num_layers, loss[gen,best]))\n",
    "    \n",
    "    model_list2 = []\n",
    "    loss_survivor = []\n",
    "    for model_ind in live:\n",
    "        model_list2.append(model_list[model_ind])\n",
    "        loss_survivor.append(loss[gen,model_ind])\n",
    "    for model_ind in live:\n",
    "        mutation = copy.deepcopy(model_list[model_ind])\n",
    "        mutation.mutate()\n",
    "        model_list2.append(mutation)\n",
    "\n",
    "    model_list = model_list2\n",
    "    with open(os.path.join(savepath,'model_list_%s_gen%d.pkl' %(model_name,gen)),'wb') as fn:\n",
    "        pickle.dump(model_list,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
